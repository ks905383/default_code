{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess CMIP6 data\n",
    "Script for downloading and saving CMIP6 files, with ability to subset by time and space. CMIP6 data is lazily loaded directly from the cloud, using the Pangeo - Google Cloud Public Dataset Program collaboration (more info [here](https://medium.com/pangeo/cmip6-in-the-cloud-five-ways-96b177abe396))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xagg as xa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cftime\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from operator import itemgetter # For list subsetting but this is idiotic\n",
    "import intake\n",
    "import gcsfs\n",
    "import os\n",
    "import warnings \n",
    "import xagg as xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs_support import (get_varlist,get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set whether to regrid 360-day calendars to 365-day calendars\n",
    "# (probably don't do this while saving files. Only do this in \n",
    "# processing code that doesn't affect the original file)\n",
    "regrid_360 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for spatiotemporal subsetting\n",
    "subset_params_all = [{'lat':[16.5,21.5],'lon':[70,75],\n",
    "                      'time':{'historical':['1958-01-01','2014-12-31'],\n",
    "                              'amip':['1958-01-01','2014-12-31'],\n",
    "                              'ssp370':['2015-01-01','2099-12-31'],\n",
    "                              'ssp585':['2015-01-01','2099-12-31']},\n",
    "                      'fn_suffix':'_Mumbai',\n",
    "                      'lon_range':180,'lon_origin':-180}]\n",
    "\n",
    "#data_params_all = [{'experiment_id':'historical','table_id':'day','variable_id':'pr'},\n",
    "#                   {'experiment_id':'ssp370','table_id':'day','variable_id':'pr'},\n",
    "#                   {'experiment_id':'ssp585','table_id':'day','variable_id':'pr'}]\n",
    "\n",
    "data_params_all = [{'experiment_id':'historical','table_id':'day','variable_id':'tas','source_id':'ACCESS-CM2'},\n",
    "                   {'experiment_id':'ssp370','table_id':'day','variable_id':'tas','source_id':'ACCESS-CM2'},\n",
    "                   {'experiment_id':'ssp585','table_id':'day','variable_id':'tas','source_id':'ACCESS-CM2'}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the full query for all the datasets that will end up getting use in this \n",
    "# process - this is to create the master dataset, so to build up the 'model' and \n",
    "# 'experiment' dimension in the dataset with all the values that will end up used\n",
    "source_calls = np.zeros(len(data_params_all[0].keys()))\n",
    "\n",
    "for key in data_params_all[0].keys():\n",
    "    if len(np.unique([x[key] for x in data_params_all]))==1:\n",
    "        source_calls[list(data_params_all[0].keys()).index(key)] = 1\n",
    "        \n",
    "# To skip the 'other' one in the join below, hopefully it works.\n",
    "\n",
    "\n",
    "# First get all the ones with the same value for each key \n",
    "subset_query = ' and '.join([k+\" == '\"+data_params_all[0][k]+\"'\" for k in itemgetter(*source_calls.nonzero()[0])(list(data_params_all[0].keys())) if k != 'other'])\n",
    "\n",
    "# Now add all that are different between subset params - i.e. those that need an OR statement\n",
    "# These have to be in two statements, because if there's only one OR'ed statement, then the \n",
    "# for k in statement goes through the letters instead of the keys. \n",
    "if len((source_calls-1).nonzero()[0])==1:\n",
    "    subset_query=subset_query+' and ('+') and ('.join([' or '.join([k+\" == '\"+data_params[k]+\"'\" for data_params in data_params_all]) \n",
    "              for k in [itemgetter(*(source_calls-1).nonzero()[0])(list(data_params_all[0].keys()))] if k != 'other'])+')'\n",
    "elif len((source_calls-1).nonzero()[0])>1:\n",
    "    subset_query=subset_query+' and ('+') and ('.join([' or '.join([k+\" == '\"+data_params[k]+\"'\" for data_params in data_params_all]) \n",
    "              for k in itemgetter(*(source_calls-1).nonzero()[0])(list(data_params_all[0].keys())) if k != 'other'])+')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access google cloud storage links\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only')\n",
    "# Get info about CMIP6 datasets\n",
    "cmip6_datasets = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "# Get subset based on the data params above (for all search parameters)\n",
    "cmip6_sub = cmip6_datasets.query(subset_query)\n",
    "\n",
    "if len(cmip6_sub) == 0:\n",
    "    warnings.warn('Query unsuccessful, no files found! Check to make sure your table_id matches the domain - SSTs are listed as \"Oday\" instead of \"day\" for example')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>377958</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>ssp370</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379386</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20191108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391447</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r2i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20191125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423702</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>ssp370</td>\n",
       "      <td>r2i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424456</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>ssp585</td>\n",
       "      <td>r2i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425192</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>historical</td>\n",
       "      <td>r3i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439379</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>ssp585</td>\n",
       "      <td>r3i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439638</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>ssp370</td>\n",
       "      <td>r3i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515792</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CSIRO-ARCCSS</td>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>ssp585</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>day</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20210317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        activity_id institution_id   source_id experiment_id member_id  \\\n",
       "377958  ScenarioMIP   CSIRO-ARCCSS  ACCESS-CM2        ssp370  r1i1p1f1   \n",
       "379386         CMIP   CSIRO-ARCCSS  ACCESS-CM2    historical  r1i1p1f1   \n",
       "391447         CMIP   CSIRO-ARCCSS  ACCESS-CM2    historical  r2i1p1f1   \n",
       "423702  ScenarioMIP   CSIRO-ARCCSS  ACCESS-CM2        ssp370  r2i1p1f1   \n",
       "424456  ScenarioMIP   CSIRO-ARCCSS  ACCESS-CM2        ssp585  r2i1p1f1   \n",
       "425192         CMIP   CSIRO-ARCCSS  ACCESS-CM2    historical  r3i1p1f1   \n",
       "439379  ScenarioMIP   CSIRO-ARCCSS  ACCESS-CM2        ssp585  r3i1p1f1   \n",
       "439638  ScenarioMIP   CSIRO-ARCCSS  ACCESS-CM2        ssp370  r3i1p1f1   \n",
       "515792  ScenarioMIP   CSIRO-ARCCSS  ACCESS-CM2        ssp585  r1i1p1f1   \n",
       "\n",
       "       table_id variable_id grid_label  \\\n",
       "377958      day         tas         gn   \n",
       "379386      day         tas         gn   \n",
       "391447      day         tas         gn   \n",
       "423702      day         tas         gn   \n",
       "424456      day         tas         gn   \n",
       "425192      day         tas         gn   \n",
       "439379      day         tas         gn   \n",
       "439638      day         tas         gn   \n",
       "515792      day         tas         gn   \n",
       "\n",
       "                                                   zstore  dcpp_init_year  \\\n",
       "377958  gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...             NaN   \n",
       "379386  gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/...             NaN   \n",
       "391447  gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/...             NaN   \n",
       "423702  gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...             NaN   \n",
       "424456  gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...             NaN   \n",
       "425192  gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/...             NaN   \n",
       "439379  gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...             NaN   \n",
       "439638  gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...             NaN   \n",
       "515792  gs://cmip6/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCE...             NaN   \n",
       "\n",
       "         version  \n",
       "377958  20191108  \n",
       "379386  20191108  \n",
       "391447  20191125  \n",
       "423702  20200303  \n",
       "424456  20200303  \n",
       "425192  20200306  \n",
       "439379  20200428  \n",
       "439638  20200428  \n",
       "515792  20210317  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmip6_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9412d272685942108a53b59e70e87d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_historical_r1i1p1f1_19580101-20141231_Mumbai.nc processed!\n",
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_historical_r2i1p1f1_19580101-20141231_Mumbai.nc processed!\n",
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_historical_r3i1p1f1_19580101-20141231_Mumbai.nc processed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fda1e690e1403ba42c1eba59abdec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_ssp370_r1i1p1f1_20150101-20991231_Mumbai.nc processed!\n",
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_ssp370_r2i1p1f1_20150101-20991231_Mumbai.nc processed!\n",
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_ssp370_r3i1p1f1_20150101-20991231_Mumbai.nc processed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af12b3b79fc49f3ae1e37248b50274b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_ssp585_r2i1p1f1_20150101-20991231_Mumbai.nc processed!\n",
      "processing ACCESS-CM2!\n",
      "/dx01/kschwarz/project_data/mumbai_projs/ACCESS-CM2/tas_day_ACCESS-CM2_ssp585_r3i1p1f1_20150101-20991231_Mumbai.nc processed!\n",
      "processing ACCESS-CM2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kschwarz/.conda/envs/climate/lib/python3.10/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/kschwarz/.conda/envs/climate/lib/python3.10/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/kschwarz/.conda/envs/climate/lib/python3.10/site-packages/xarray/core/indexing.py:529: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return np.asarray(array[self.key], dtype=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time dimension is 0, skipping\n"
     ]
    }
   ],
   "source": [
    "#------ Process by variable and dataset in the subset ------\n",
    "overwrite=False\n",
    "for data_params in data_params_all:\n",
    "    # Get subset based on the data params above, now just for this one variable\n",
    "    cmip6_sub = cmip6_datasets.query(' and '.join([k+\" == '\"+data_params[k]+\"'\" for k in data_params.keys() if k != 'other']))\n",
    "         \n",
    "        \n",
    "    for url in tqdm(cmip6_sub.zstore.values):\n",
    "        print('processing '+url.split('/')[6]+'!')\n",
    "        try:\n",
    "            # Set output filenames\n",
    "            output_fns = [None]*len(subset_params_all)\n",
    "            path_exists = [None]*len(subset_params_all)\n",
    "            for subset_params in subset_params_all:\n",
    "                if 'time' in subset_params:\n",
    "                    time_str = '-'.join([re.sub('-','',t) for t in subset_params['time'][data_params['experiment_id']]])\n",
    "                else:\n",
    "                    time_str = ''\n",
    "                    \n",
    "                if 'member_id' in data_params:\n",
    "                    member_id = data_params['member_id']\n",
    "                else:\n",
    "                    member_id = url.split('/')[8]\n",
    "                output_fns[subset_params_all.index(subset_params)] = (dir_list['raw']+url.split('/')[6]+'/'+\n",
    "                                                                     data_params['variable_id']+'_'+\n",
    "                                                                     data_params['table_id']+'_'+url.split('/')[6]+'_'+\n",
    "                                                                     data_params['experiment_id']+'_'+member_id+'_'+\n",
    "                                                                     time_str+\n",
    "                                                                     subset_params['fn_suffix']+'.nc')\n",
    "\n",
    "                if 'other' in data_params.keys(): \n",
    "                    if 'plev_subset' in data_params['other'].keys():\n",
    "                        output_fns[subset_params_all.index(subset_params)] = re.sub(data_params['variable_id'],\n",
    "                                                                                data_params['other']['plev_subset']['outputfn'],\n",
    "                                                                               output_fns[subset_params_all.index(subset_params)])\n",
    "\n",
    "\n",
    "\n",
    "                path_exists[subset_params_all.index(subset_params)] = os.path.exists(output_fns[subset_params_all.index(subset_params)])\n",
    "\n",
    "            if (not overwrite) & all(path_exists):\n",
    "                warnings.warn('All files already created for '+data_params['variable_id']+' '+\n",
    "                                                                     data_params['table_id']+' '+url.split('/')[6]+' '+\n",
    "                                                                     data_params['experiment_id']+' '+member_id+', skipped.')\n",
    "                continue\n",
    "            elif any(path_exists):\n",
    "                if overwrite:\n",
    "                    for subset_params in subset_params_all:\n",
    "                        if path_exists[subset_params_all.index(subset_params)]:\n",
    "                            os.remove(output_fns[subset_params_all.index(subset_params)])\n",
    "                            warnings.warn('All files already exist for '+data_params['variable_id']+' '+\n",
    "                                                                                 data_params['table_id']+' '+url.split('/')[6]+' '+\n",
    "                                                                                 data_params['experiment_id']+' '+member_id+\n",
    "                                          ', because OVERWRITE=TRUE these files have been deleted.')\n",
    "\n",
    "\n",
    "            # Open dataset\n",
    "            ds = xr.open_zarr(fs.get_mapper(url),consolidated=True)\n",
    "\n",
    "            # Rename to lat / lon (let's hope there's no \n",
    "            # Latitude / latitude_1 / etc. in this dataset)\n",
    "            try:\n",
    "                ds = ds.rename({'longitude':'lon','latitude':'lat'})\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "            # same with 'nav_lat' and 'nav_lon' ???\n",
    "            try:\n",
    "                ds = ds.rename({'nav_lon':'lon','nav_lat':'lat'})\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "            # If precip, kg/m^2/s, switch to mm/day\n",
    "            #if data_params['variable_id']=='pr':\n",
    "            #    ds[data_params['variable_id']] = ds[data_params['variable_id']]*60*60*24\n",
    "\n",
    "            # Fix coordinate doubling (this was an issue in NorCPM1, \n",
    "            # where thankfully the values of the variables were nans,\n",
    "            # though I still don't know how this happened - some lat\n",
    "            # values were doubled within floating point errors)\n",
    "            if 'lat' in ds[data_params['variable_id']].dims:\n",
    "                if len(np.unique(np.round(ds.lat.values,10))) != ds.dims['lat']:\n",
    "                    ds = ds.isel(lat=(~np.isnan(ds.isel(lon=1,time=1)[data_params['variable_id']].values)).nonzero()[0],drop=True)\n",
    "                    warnings.warn('Model '+ds.source_id+' has duplicate lat values; attempting to compensate by dropping lat values that are nan in the main variable in the first timestep')\n",
    "                if len(np.unique(np.round(ds.lon.values,10))) != ds.dims['lon']:\n",
    "                    ds = ds.isel(lon=(~np.isnan(ds.isel(lat=1,time=1)[data_params['variable_id']].values)).nonzero()[0],drop=True)\n",
    "                    warnings.warn('Model '+ds.source_id+' has duplicate lon values; attempting to compensate by dropping lon values that are nan in the main variable in the first timestep')\n",
    "\n",
    "            # Sort by time, if not sorted (this happened with\n",
    "            # a model; keeping a warning, cuz this seems weird)\n",
    "            if 'time' in subset_params:\n",
    "                if (ds.time.values != np.sort(ds.time)).any():\n",
    "                    warnings.warn('Model '+ds.source_id+' has an unsorted time dimension.')\n",
    "                    ds = ds.sortby('time')\n",
    "\n",
    "            # If 360-day calendar, regrid to 365-day calendar\n",
    "            if regrid_360:\n",
    "                if ds.dims['dayofyear'] == 360:\n",
    "                    # Have to put in the compute() because these \n",
    "                    # are by default dask arrays, chunked along\n",
    "                    # the time dimension, and can't interpolate\n",
    "                    # across dask chunks... \n",
    "                    ds = ds.compute().interp(dayofyear=(np.arange(1,366)/365)*360)\n",
    "                    # And reset it to 1:365 indexing on day of year\n",
    "                    ds['dayofyear'] = np.arange(1,366)\n",
    "                    # Throw in a warning, too, why not\n",
    "                    warnings.warn('Model '+ds.source_id+' has a 360-day calendar; daily values were interpolated to a 365-day calendar')\n",
    "\n",
    "            # Now, save by the subsets desired in subset_params_all above\n",
    "            for subset_params in subset_params_all:\n",
    "                # Make sure this file hasn't already been processed\n",
    "                if (not overwrite) & path_exists[subset_params_all.index(subset_params)]:\n",
    "                    warnings.warn(output_fns[subset_params_all.index(subset_params)]+' already exists; skipped.')\n",
    "                    continue\n",
    "\n",
    "                # Make sure the target directory exists\n",
    "                if not os.path.exists(dir_list['raw']+url.split('/')[6]+'/'):\n",
    "                    os.mkdir(dir_list['raw']+url.split('/')[6]+'/')\n",
    "                    warnings.warn('Directory '+dir_list['raw']+url.split('/')[6]+'/'+' created!')\n",
    "\n",
    "                # Fix longitude (by setting it to either [-180:180] \n",
    "                # or [0:360] as determined by subset_params, and \n",
    "                # to roll them so the correct range is consecutive \n",
    "                # in lon (so if you're looking at the Equatorial \n",
    "                # Pacific, make it 0:360, with the first lon value\n",
    "                # at 45E). \n",
    "                if 'lat' in ds[data_params['variable_id']].dims:\n",
    "                    ds_tmp = xa.fix_ds(ds,subset_params)\n",
    "                    # Now, cutoff the values below the 'lon_origin', \n",
    "                    # because slice doesn't work if the indices aren't\n",
    "                    # montonically increasing (and the above changes it\n",
    "                    # to [lon_origin:360 0:lon_origin]\n",
    "                    if np.abs(ds_tmp.lon[0]-subset_params['lon_origin'])>5:\n",
    "                        ds_tmp = ds_tmp.isel(lon=np.arange(0,(ds_tmp.lon // (subset_params['lon_origin']) == 0).values.nonzero()[0][0]))\n",
    "                else:\n",
    "                    ds_tmp = ds.copy()\n",
    "                    warnings.warn('fix_ds did not work because of the multi-dimensional index')\n",
    "\n",
    "                # Subset by time as set in subset_params\n",
    "                if 'time' in subset_params:\n",
    "                    if (ds.time.max().dt.day==30) | (type(ds.time.values[0]) == cftime._cftime.Datetime360Day): \n",
    "                        # (If it's a 360-day calendar, then subsetting to \"12-31\"\n",
    "                        # will throw an error; this switches that call to \"12-30\")\n",
    "                        # Also checking explicitly for 360day calendar; some monthly \n",
    "                        # data is still shown as 360-day even when it's monthly, and will\n",
    "                        # fail on date ranges with date 31 in a month\n",
    "                        ds_tmp = (ds_tmp.sel(time=slice(subset_params['time'][data_params['experiment_id']][0],\n",
    "                                                re.sub('-31','-30',subset_params['time'][data_params['experiment_id']][1]))))\n",
    "                    else:\n",
    "                        ds_tmp = (ds_tmp.sel(time=slice(*subset_params['time'][data_params['experiment_id']])))\n",
    "\n",
    "                # Subset by space as set in subset_params\n",
    "                if 'lat' in subset_params.keys():\n",
    "                    if not 'lat' in ds[data_params['variable_id']].dims:\n",
    "                        ds_tmp = ds_tmp.where((ds_tmp.lat >= subset_params['lat'][0]) & (ds_tmp.lat <= subset_params['lat'][1]) &\n",
    "                         (ds_tmp.lon >= subset_params['lon'][0]) & (ds_tmp.lon <= subset_params['lon'][1]),drop=True)\n",
    "                    else:\n",
    "                        ds_tmp = (ds_tmp.sel(lat=slice(*subset_params['lat']),\n",
    "                                             lon=slice(*subset_params['lon'])))\n",
    "\n",
    "                # If subsetting by pressure level...\n",
    "                if 'other' in data_params.keys():\n",
    "                    if 'plev_subset' in data_params['other'].keys():\n",
    "                        # Have to use np.allclose for floating point errors\n",
    "                        try:\n",
    "                            ds_tmp = ds_tmp.isel(plev=np.where([np.allclose(p,data_params['other']['plev_subset']['plev']) for p in ds_tmp.plev])[0][0])\n",
    "                            ds_tmp = ds_tmp.rename({data_params['variable_id']:data_params['other']['plev_subset']['outputfn']})\n",
    "                        except KeyError:\n",
    "                            print('The pressure levels: ')\n",
    "                            print(ds_tmp.plev.values)\n",
    "                            print(' do not contain '+str(data_params['other']['plev_subset']['plev'])+'; skipping.')\n",
    "                            del ds_tmp\n",
    "                            continue\n",
    "                # Trick to make the loading process go faster (otherwise it\n",
    "                # gets stuck forever in .to_netcdf below; and .load() is \n",
    "                # just as slow for some reason)\n",
    "                #if 'time' in subset_params:\n",
    "                    #tmp = ds_tmp.mean('time')\n",
    "                    #del tmp\n",
    "\n",
    "                # Save as NetCDF file\n",
    "                if ds_tmp.dims['time']>0:\n",
    "                    try:\n",
    "                        ds_tmp.to_netcdf(output_fns[subset_params_all.index(subset_params)])\n",
    "                    except ValueError:\n",
    "                        print('issue with export; skipping')\n",
    "                        #del ds_tmp\n",
    "                        continue\n",
    "                else:\n",
    "                    print('time dimension is 0, skipping')\n",
    "                    continue\n",
    "                    \n",
    "\n",
    "                # Status update\n",
    "                print(output_fns[subset_params_all.index(subset_params)]+' processed!')\n",
    "\n",
    "            del ds, ds_tmp, subset_params\n",
    "        except AssertionError:\n",
    "            print('checksum error with model '+url.split('/')[6]+', skipping for now.')\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
